{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len([1,2]), 5):\n",
    "    print([1,2][i : i + 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from openai import APITimeoutError, InternalServerError, BadRequestError, RateLimitError, UnprocessableEntityError\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    retry_if_exception_type,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    ")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(min=2, max=5),\n",
    "    retry=retry_if_exception_type(\n",
    "        (\n",
    "            APITimeoutError,\n",
    "            InternalServerError,\n",
    "            RateLimitError,\n",
    "            UnprocessableEntityError\n",
    "        )\n",
    "    )\n",
    ")\n",
    "def say_hello():\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"HELLO\"}\n",
    "        ]\n",
    "    )\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8iKsIOOKt9H0rrdoNfngIbWps4Q4k', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705577366, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iKsIT894HjftXI1SCwzAtoh9ZlyX', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705577366, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iKsNUcEPQrFk5LXpSnNw8azIyERk', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705577371, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iKsRIjJYC3aBvIAskDerrNaaIoIC', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705577375, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iKsR7fbY6noVakyg87lGglIeu1JX', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hi there! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705577375, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=10, prompt_tokens=9, total_tokens=19))\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    say_hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=[\"Hello\"]*20,\n",
    "    max_tokens=20,\n",
    ")\n",
    "# for choice in response.choices:\n",
    "#     print(choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=388, prompt_tokens=20, total_tokens=408)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "# for _ in range(5):\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Hello\",\n",
    "    max_tokens=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=20, prompt_tokens=1, total_tokens=21)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "async_client = AsyncOpenAI()\n",
    "\n",
    "async def async_say_hello():\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"HELLO\"}\n",
    "        ]\n",
    "    )\n",
    "    print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "for f in tqdm.as_completed([async_say_hello() for _ in range(5)]):\n",
    "    await f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    await asyncio.gather(*[async_say_hello() for _ in range(20)])\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_batches(items: list, batch_size: int = 5):\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run():\n",
    "    calls = [async_say_hello() for _ in range(15)]\n",
    "    for batch in _generate_batches(calls):\n",
    "        print(\"running batch\")\n",
    "        for future in asyncio.as_completed(batch):\n",
    "            await future\n",
    "            \n",
    "asyncio.run(run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run():\n",
    "    calls = [async_say_hello() for _ in range(15)]\n",
    "    for batch in _generate_batches(calls):\n",
    "        print(\"running batch\")\n",
    "        for future in asyncio.as_completed(batch):\n",
    "            yield await future\n",
    "            # yield result  # yield the result instead of returning it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<async_generator object run at 0x10aaacc80>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running batch\n",
      "ChatCompletion(id='chatcmpl-8iPqcvFDJGnlSmsv18ACdFNsrvrzX', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596482, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPqc2XLtDwpbrR94GC8Svu4FY1wq', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596482, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqcvFDJGnlSmsv18ACdFNsrvrzX', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596482, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqc2XLtDwpbrR94GC8Svu4FY1wq', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596482, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPqckU313M5DIImTtyaTzrEStt5M', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596482, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqckU313M5DIImTtyaTzrEStt5M', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596482, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPqcOf3W88xKPfpRuFmdnEkQGpEB', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596482, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqcOf3W88xKPfpRuFmdnEkQGpEB', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596482, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPqcVEpZedIBaORyPEVjZIPM4OTE', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596482, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqcVEpZedIBaORyPEVjZIPM4OTE', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596482, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "running batch\n",
      "ChatCompletion(id='chatcmpl-8iPqhbPSslCriP5KmmyUK65XYiuUz', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hi! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596487, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqhbPSslCriP5KmmyUK65XYiuUz', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hi! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596487, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPqhothYsBg2P0txGfzNx8lpw9ad', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596487, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqhothYsBg2P0txGfzNx8lpw9ad', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596487, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPqhFD4Be6wSflijNAaaNBBNEcj2', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello there! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596487, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=10, prompt_tokens=9, total_tokens=19))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqhFD4Be6wSflijNAaaNBBNEcj2', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello there! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596487, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=10, prompt_tokens=9, total_tokens=19))\n",
      "ChatCompletion(id='chatcmpl-8iPqhU8e9Xko3yr5Rmt5DMxxHTttV', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596487, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqhU8e9Xko3yr5Rmt5DMxxHTttV', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596487, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPqhI73Cj8W8VgaCwdO9YqD3EIDQ', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596487, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqhI73Cj8W8VgaCwdO9YqD3EIDQ', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596487, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "running batch\n",
      "ChatCompletion(id='chatcmpl-8iPqnc9aaIGyIGYe6xurWv2FpnFIj', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596493, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqnc9aaIGyIGYe6xurWv2FpnFIj', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596493, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPqnCmYPeeS91KoFCpThrTTZQ4wE', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596493, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqnCmYPeeS91KoFCpThrTTZQ4wE', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596493, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPqniWYMA72UGFOq7XJEvrfOo5wG', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596493, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqniWYMA72UGFOq7XJEvrfOo5wG', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596493, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPqntEnMeaXeraZwGpDVpO7xRMSe', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596493, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqntEnMeaXeraZwGpDVpO7xRMSe', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596493, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPqnhjTpOCnpHIqChI7BQZYgYyrH', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596493, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPqnhjTpOCnpHIqChI7BQZYgYyrH', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705596493, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n"
     ]
    }
   ],
   "source": [
    "async for result in run():\n",
    "    print(\"RESULT\",result)  # process result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "import logging\n",
    "from openai import AsyncOpenAI\n",
    "from openai import APITimeoutError, InternalServerError, BadRequestError, RateLimitError, UnprocessableEntityError\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    retry_if_exception_type,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    ")\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "    \n",
    "class Llm(BaseModel):\n",
    "    model: str = \"gpt-3.5-turbo-instruct\"\n",
    "    batch_size: int = 10\n",
    "    use_legacy: bool = True\n",
    "    \n",
    "    async def run_batch_completions(self, prompts, **kwargs):\n",
    "        calls = [\n",
    "            self._run_completions(prompt, **kwargs) \n",
    "            for prompt in prompts\n",
    "        ]\n",
    "        for batch in self._batches(calls):\n",
    "            for future in asyncio.as_completed(batch):\n",
    "                yield await future\n",
    "    \n",
    "    @retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(min=2, max=5),\n",
    "        retry=retry_if_exception_type(\n",
    "            (\n",
    "                APITimeoutError,\n",
    "                InternalServerError,\n",
    "                RateLimitError,\n",
    "                UnprocessableEntityError\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    async def _run_completions(self, prompt, **kwargs):\n",
    "        if self.use_legacy:\n",
    "            return await self._run_legacy_completions(prompt, **kwargs)\n",
    "        else:\n",
    "            return await self._run_chat_completions(prompt, **kwargs)\n",
    "    \n",
    "    async def _run_legacy_completions(self, prompt, **kwargs):\n",
    "        logging.info(\"legacy completions uses 'gpt-3.5-turbo-instruct' model\")\n",
    "        return await client.completions.create(\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            prompt=prompt,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "    async def _run_chat_completions(self, prompt, **kwargs):\n",
    "        messages = {\"user\": \"role\", \"content\": prompt}\n",
    "        return await client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def _batches(self, prompts: list):\n",
    "        if self.use_legacy:\n",
    "            # can send 20 prompts at once with legacy completions. \n",
    "            # See: https://platform.openai.com/docs/guides/rate-limits/batching-requests\n",
    "            prebatched_prompts = [\n",
    "                prompt_batch for prompt_batch in self._generate_batches(prompts, 20)\n",
    "            ]\n",
    "            yield from self._generate_batches(prebatched_prompts, self.batch_size)\n",
    "        else:\n",
    "            yield from self._generate_batches(prompts, self.batch_size)\n",
    "    \n",
    "    def _generate_batches(self, prompts, batch_size):\n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            yield prompts[i : i + batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<coroutine object Llm._run_completions at 0x10b775210>, <coroutine object Llm._run_completions at 0x10b7746d0>, <coroutine object Llm._run_completions at 0x10b775300>, <coroutine object Llm._run_completions at 0x10b7745e0>, <coroutine object Llm._run_completions at 0x10b7754e0>, <coroutine object Llm._run_completions at 0x10b774040>, <coroutine object Llm._run_completions at 0x10b774130>, <coroutine object Llm._run_completions at 0x10b774220>, <coroutine object Llm._run_completions at 0x10b774a90>, <coroutine object Llm._run_completions at 0x10b774b80>]\n"
     ]
    }
   ],
   "source": [
    "llm = Llm()\n",
    "prompts = [\"hello world\"] * 10\n",
    "calls = [\n",
    "    llm._run_completions(prompt) \n",
    "    for prompt in prompts\n",
    "]\n",
    "for batch in llm._generate_batches(calls, llm.batch_size):\n",
    "    print(batch)\n",
    "    for future in asyncio.as_completed(batch):\n",
    "        await future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running batch\n",
      "ChatCompletion(id='chatcmpl-8iPiZvyPnY9eavml1Ritj3Wm5J5Gq', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595983, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPiZvyPnY9eavml1Ritj3Wm5J5Gq', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595983, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPiZt8GUuHx1K0rnirMFKWsWrkVf', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595983, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPiZt8GUuHx1K0rnirMFKWsWrkVf', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595983, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPiZW6KPG7dnqiUeoqXt4qP8uDQk', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595983, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPiZW6KPG7dnqiUeoqXt4qP8uDQk', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595983, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPiZ7QU4kiF6sdZmCKsdZYCv5io9', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595983, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPiZ7QU4kiF6sdZmCKsdZYCv5io9', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595983, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPiZNNqn1KbtOJzm7EvIMwkLcHI4', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595983, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPiZNNqn1KbtOJzm7EvIMwkLcHI4', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595983, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "running batch\n",
      "ChatCompletion(id='chatcmpl-8iPidU0A33sbMgUWTmlIASPxQpYLE', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595987, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPidU0A33sbMgUWTmlIASPxQpYLE', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595987, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPidS0cEtiWa1hIZknVebX80U7OD', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595987, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPidS0cEtiWa1hIZknVebX80U7OD', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595987, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPidZX5cpmalJtuLfAgcyvHdbXJh', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How are you doing today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595987, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=8, prompt_tokens=9, total_tokens=17))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPidZX5cpmalJtuLfAgcyvHdbXJh', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How are you doing today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595987, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=8, prompt_tokens=9, total_tokens=17))\n",
      "ChatCompletion(id='chatcmpl-8iPid3tWNZQvkMelGzGw6sINwDlsd', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello there! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595987, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=10, prompt_tokens=9, total_tokens=19))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPid3tWNZQvkMelGzGw6sINwDlsd', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello there! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595987, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=10, prompt_tokens=9, total_tokens=19))\n",
      "ChatCompletion(id='chatcmpl-8iPidPiJwDCdZIKLqqR54ERUYQlkM', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595987, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPidPiJwDCdZIKLqqR54ERUYQlkM', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595987, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "running batch\n",
      "ChatCompletion(id='chatcmpl-8iPijQxMjn6Cmwt1g8nSfbmAtFoQ6', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595993, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPijQxMjn6Cmwt1g8nSfbmAtFoQ6', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595993, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPijVcKBrVAIiPoNBrxzjdvvT5Nv', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595993, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPijVcKBrVAIiPoNBrxzjdvvT5Nv', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595993, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPij9ihADdRdkO7aMWZKbiS2gXyc', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595993, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPij9ihADdRdkO7aMWZKbiS2gXyc', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595993, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPijwH0FuW2qxv1DIMiFWcn0h2GE', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595993, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPijhNeaWvwhtWWlfjITJEKAN8mj', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595993, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPijwH0FuW2qxv1DIMiFWcn0h2GE', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595993, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "RESULT ChatCompletion(id='chatcmpl-8iPijhNeaWvwhtWWlfjITJEKAN8mj', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595993, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    async for result in run():\n",
    "        print(\"RESULT\",result)  # process result\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running batch\n",
      "ChatCompletion(id='chatcmpl-8iPcmYXrvlc7iB0ExMGQjcJIJuD63', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595624, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPcmhSsPNiEWR1ZuFBk2gWgKKZFe', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595624, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPcm0qtYmJ7oHjUrQKNvqnMrcbKi', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595624, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPcmsX0NLMZoGkglJOt2JU9Hg3cu', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595624, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPcm8XJlQo3tPuyVIZMfPD5xYGRB', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello there! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595624, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=10, prompt_tokens=9, total_tokens=19))\n",
      "running batch\n",
      "ChatCompletion(id='chatcmpl-8iPcriHPkZJhdpEqeh0kW2NPowk0a', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595629, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPcr6PBpDmnmjHLn5gDH7a7qeChH', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595629, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPcr3Dv8DQTvCPYT9OoMjdjpO4CQ', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595629, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPcrIC0b9gDRJLDd9vfbxQSoytOm', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595629, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPcrjbF1PQnoLjkfC9N6Mi2t10MW', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595629, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "running batch\n",
      "ChatCompletion(id='chatcmpl-8iPcyoCxhPxLskAjk7YBNwHotWgHB', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595636, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPcyb16rtMYAKhlm3kCeYkIEdwv4', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595636, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPcyXhrLf0Tt9JmTCj0m33DKnafO', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595636, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n",
      "ChatCompletion(id='chatcmpl-8iPcyN3izBcJf1ajQq7nYlYe0X0vd', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello there! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595636, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=10, prompt_tokens=9, total_tokens=19))\n",
      "ChatCompletion(id='chatcmpl-8iPcyt1WmzsbpgpLbQM4deNEEGIuG', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I help you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1705595636, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_fe56e538d5', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(run())\n",
    "    # print(f\"RESULTS: {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calls = [async_say_hello() for _ in range(15)]\n",
    "for batch in _generate_batches(calls):\n",
    "    print(\"running batch\")\n",
    "    await asyncio.gather(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    await asyncio.gather(*[async_say_hello() for _ in range(15)])\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = cb.get_functions(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Explain in MAXIMUM 10 TOKENS what the following function does:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt += functions[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sets system prompt as message in model instantiation.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "#   response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INFO = {\n",
    "    \"gpt-4-1106-preview\": {\"context\": 128192, \"inprice\": 0.01, \"outprice\": 0.03},\n",
    "    \"gpt-4\": {\"context\": 8192, \"inprice\": 0.03, \"outprice\": 0.06},\n",
    "    \"gpt-4-0613\": {\"context\": 8192, \"inprice\": 0.03, \"outprice\": 0.06},\n",
    "    \"gpt-4-32k\": {\"context\": 32000, \"inprice\": 0.06, \"outprice\": 0.12},\n",
    "    \"gpt-4-32k-0613\": {\"context\": 32000, \"inprice\": 0.06, \"outprice\": 0.12},\n",
    "    \"gpt-3.5-turbo-1106\": {\"context\": 16385, \"inprice\": 0.0010, \"outprice\": 0.0020},\n",
    "    \"gpt-3.5-turbo-instruct\": {\"context\": 4096, \"inprice\": 0.0015, \"outprice\": 0.0020},\n",
    "    \"gpt-3.5-turbo\": {\"context\": 4096, \"inprice\": 0.0015, \"outprice\": 0.0020},\n",
    "    \"gpt-3.5-turbo-0613\": {\"context\": 4096, \"inprice\": 0.0015, \"outprice\": 0.0020},\n",
    "    \"gpt-3.5-turbo-16k\": {\"context\": 16385, \"inprice\": 0.0030, \"outprice\": 0.0040},\n",
    "    \"gpt-3.5-turbo-16k-0613\": {\"context\": 16385, \"inprice\": 0.0030, \"outprice\": 0.0040},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cost(intokens, outtokens, model):\n",
    "    return round((\n",
    "        intokens * MODEL_INFO[model][\"inprice\"] \n",
    "        + outtokens * MODEL_INFO[model][\"outprice\"]\n",
    "    ) / 1000, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def count_tokens(text: str, model: str):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cost(cb, model):\n",
    "    cost = 0\n",
    "    for path in cb.get_modules_paths():\n",
    "        for function in cb.get_functions(path):\n",
    "            cost += calc_cost(intokens=count_tokens(function.content, model), outtokens=10, model=model)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008659"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_cost(cb,model=\"gpt-3.5-turbo-1106\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "\n",
    "for path in cb.get_modules_paths():\n",
    "    for function in cb.get_functions(path):\n",
    "        prompt = \"\"\"\n",
    "        Explain in MAXIMUM 10 TOKENS what the following function does:\n",
    "        \"\"\"\n",
    "        prompt += function.content\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "          model=\"gpt-3.5-turbo-1106\",\n",
    "          messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "          ]\n",
    "        )\n",
    "        metadata.append(\n",
    "          {\n",
    "            \"path\": path, \n",
    "            \"function\": function.name, \n",
    "            \"code\": function.content, \n",
    "            \"response\": response.choices[0].message.content}\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeas.utils import write_yaml\n",
    "\n",
    "write_yaml(\"metadata.yaml\", {\"output\":metadata})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = cb.get_functions(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_lines(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_file_lines(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_line(identifier, include_args=False):\n",
    "    if include_args:\n",
    "        NotImplementedError()\n",
    "    else:\n",
    "        return lines[identifier.start_point[0]][:identifier.end_point[1]]\n",
    "\n",
    "def get_name(identifier):\n",
    "    return identifier.text.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identifier(node):\n",
    "    for child in node.children:\n",
    "        if child.type == \"identifier\":\n",
    "            return child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_function_line(node):\n",
    "    identifier = get_identifier(node)\n",
    "    return get_file_line(identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for path in cb.get_modules_paths():\n",
    "    for function in cb.get_functions(path):\n",
    "        lines.append(function.node.start_point[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/manuelrenner/repos/codeas/src/codeas/thread.py'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.abspath(metadata[0][\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absolute_path(path):\n",
    "    return os.path.abspath(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_map = {}\n",
    "for function, line in zip(metadata, lines):\n",
    "    id_ = function[\"function\"] + \"_\" + str(line)\n",
    "    abs_path = get_absolute_path(function[\"path\"])\n",
    "    if abs_path not in repo_map:\n",
    "        repo_map[abs_path] = {\n",
    "            id_: {\n",
    "                \"name\": function[\"function\"],\n",
    "                \"line\": line,\n",
    "                \"type\": \"function_definition\",\n",
    "                \"short_desc\": function[\"response\"]\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        repo_map[abs_path][id_] = {\n",
    "            \"name\": function[\"function\"],\n",
    "            \"line\": line,\n",
    "            \"type\": \"function_definition\",\n",
    "            \"short_desc\": function[\"response\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"repo_map.json\", \"w\") as f:\n",
    "    json.dump(repo_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_map['../../codeas/src/codeas/thread.py']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Node type=function_definition, start_point=(38, 4), end_point=(41, 79)>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions[0].node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(path, node):\n",
    "    id_ = get_name(get_identifier(node)) + \"_\" + str(node.start_point[0])\n",
    "    return repo_map[path][id_][\"short_desc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 22\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[38;5;66;03m# if add_description:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m                     \u001b[38;5;66;03m#     file_structure += \"():\\n\\t... # \" + get_description(path, grandchild) + \"\\n\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m                     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     19\u001b[0m                     \u001b[38;5;66;03m#     file_structure += \"():\\n\\t...\\n\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file_structure\n\u001b[0;32m---> 22\u001b[0m \u001b[43mget_file_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../codeas/src/codeas/thread.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[180], line 12\u001b[0m, in \u001b[0;36mget_file_structure\u001b[0;34m(path, add_description)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# if add_description:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#     file_structure += \"():\\n\\t... # \" + get_description(path, child) + \"\\n\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#     file_structure += \"():\\n\\t...\\n\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m child\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_definition\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     file_structure \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mget_function_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m():\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m grandchild \u001b[38;5;129;01min\u001b[39;00m child\u001b[38;5;241m.\u001b[39mchildren[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren: \u001b[38;5;66;03m#children[-1] is the class block\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m grandchild\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_definition\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[140], line 3\u001b[0m, in \u001b[0;36mget_function_line\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_function_line\u001b[39m(node):\n\u001b[1;32m      2\u001b[0m     identifier \u001b[38;5;241m=\u001b[39m get_identifier(node)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_file_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[135], line 5\u001b[0m, in \u001b[0;36mget_file_line\u001b[0;34m(identifier, include_args)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlines\u001b[49m\u001b[43m[\u001b[49m\u001b[43midentifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_point\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43midentifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_point\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def get_file_structure(path, add_description=True):\n",
    "    root_node =cb.parse_root_node(path)\n",
    "    file_structure = \"\"\n",
    "    for child in root_node.children:\n",
    "        if child.type == \"function_definition\":\n",
    "            file_structure += get_function_line(child) \n",
    "            # if add_description:\n",
    "            #     file_structure += \"():\\n\\t... # \" + get_description(path, child) + \"\\n\"\n",
    "            # else:\n",
    "            #     file_structure += \"():\\n\\t...\\n\"\n",
    "        elif child.type == \"class_definition\":\n",
    "            file_structure += get_function_line(child) + \"():\\n\"\n",
    "            for grandchild in child.children[-1].children: #children[-1] is the class block\n",
    "                if grandchild.type == \"function_definition\":\n",
    "                    file_structure += get_function_line(grandchild)\n",
    "                    # if add_description:\n",
    "                    #     file_structure += \"():\\n\\t... # \" + get_description(path, grandchild) + \"\\n\"\n",
    "                    # else:\n",
    "                    #     file_structure += \"():\\n\\t...\\n\"\n",
    "    return file_structure\n",
    "\n",
    "get_file_structure(\"../../codeas/src/codeas/thread.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class Thread():\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifiers = []\n",
    "for element in code_elements:\n",
    "    for child in element.children:\n",
    "        if child.type == \"identifier\":\n",
    "            identifiers.append({\"start_line\": child.start_point[0], \"end_pos\": child.end_point[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class Thread(BaseModel):\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[identifiers[0][\"start_line\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Thread\n"
     ]
    }
   ],
   "source": [
    "for identifier in identifiers:\n",
    "    print(lines[identifier[\"start_line\"]][:identifier[\"end_pos\"]])\n",
    "# identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identifier(lines, node):\n",
    "    for child_node in node.children:\n",
    "        if child_node.type == \"identifier\":\n",
    "            return child_node.start_point[0], child_node.end_point[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "line, pos = get_identifier_line(functions[0].node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    def model_post_init'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[line][:pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ../../codeas/src/codeas/thread.py\n",
      "class Thread(BaseModel):\n",
      "    def model_post_init(self, __context: Any) -> None:\n",
      "    def run(self):\n",
      "    def _add_context_to_messages(self):\n",
      "    def _remove_context_from_messages(self):\n",
      "    def trim_messages(self):\n",
      "    def _run_messages(self):\n",
      "    def check_messages_fit_context_window(self):\n",
      "    def check_codebase_context_fits(self):\n",
      "    def remove_oldest_messages(self):\n",
      "    def _run_completion(self):\n",
      "    def _parse_delta_content(self, delta, response):\n",
      "    def _print_delta_content(self, delta):\n",
      "    def _parse_delta_tools(self, delta, response):\n",
      "    def _print_delta_tools(self, response, live):\n",
      "    def run_calls(self, tool_calls: List[dict]):\n",
      "    def call(self, tool_call: dict):\n",
      "    def _get_function_args(self, tool_call: dict):\n",
      "    def _print_call(self, function, args):\n",
      "    def _print_call_success(self):\n",
      "    def _print_call_error(self, e):\n",
      "    def count_tokens_from_messages(self):\n",
      "    def count_tokens(self, text: str):\n",
      "    def add_message(self, message: dict):\n",
      "    def add_context(self, context: List[File]):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cb.get_file_structure(paths[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async\n",
      "def\n",
      "model_post_init\n",
      "(self, __context: Any)\n",
      "->\n",
      "None\n",
      ":\n",
      "# executed on model instantiation\n",
      "if self.system_prompt is not None:\n",
      "            self.add_message({\"role\": \"system\", \"content\": self.system_prompt})\n"
     ]
    }
   ],
   "source": [
    "function_def = \"\"\n",
    "for node in functions[0].node.children:\n",
    "        print(node.text.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Node type=\"async\", start_point=(38, 4), end_point=(38, 9)>,\n",
       " <Node type=\"def\", start_point=(38, 10), end_point=(38, 13)>,\n",
       " <Node type=identifier, start_point=(38, 14), end_point=(38, 29)>,\n",
       " <Node type=parameters, start_point=(38, 29), end_point=(38, 51)>,\n",
       " <Node type=\"->\", start_point=(38, 52), end_point=(38, 54)>,\n",
       " <Node type=type, start_point=(38, 55), end_point=(38, 59)>,\n",
       " <Node type=\":\", start_point=(38, 59), end_point=(38, 60)>,\n",
       " <Node type=comment, start_point=(39, 8), end_point=(39, 41)>,\n",
       " <Node type=block, start_point=(40, 8), end_point=(41, 79)>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
